---
title: "Decoding Hidden Themes: A Hyperparameter-Optimized Latent Topic Model"
author: "Siddharth Nand"
bibliography: references.bib
output:
  pdf_document:
    latex_engine: xelatex
    toc: false
    number_sections: false
    fig_caption: true
    fig_height: 5
    fig_width: 7
    highlight: haddock
header-includes:
  - \usepackage{algorithm2e}
---
```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(stringr)
library(rstan)
library(tidytext)
library(textdata)
library(SnowballC)
library(tm)
library(caret)

setwd("/Users/siddharthnand/Library/CloudStorage/OneDrive-UBC/Undergrad Courses/W2023/STAT 477C Special Topics in Statistics - BAYESIAN STATS/Project")
```

# Introduction
Latent topic modeling is a powerful unsupervised machine learning technique for discovering hidden thematic structures within text corpora. These models aim to represent documents as mixtures of topics, where each topic is a probability distribution over words. This project introduces a novel latent topic modeling approach that incorporates the following advancements:

* **Custom Likelihood Function:** A specifically designed likelihood function is used to model the probability of observing words within documents, emphasizing the relationship between word frequencies and topic assignments.
* **Flexible Parameterization:** The model incorporates hyperparameters for sparsity control (Dirichlet concentration parameter) and word frequency influence. This allows for fine-tuning the topic discovery process.
* **Hyperparameter Optimization:**  Hyperparameters are carefully tuned to maximize the quality and coherence of the discovered topics.

# Literature Review

## Latent Dirichlet Allocation
Latent Dirichlet Allocation (LDA) is a widely used probabilistic generative model in natural language processing for uncovering the latent thematic structure within a collection of documents. First proposed in the context of population genetics [@Pritchard2000; @Falush2003], LDA posits a hierarchical Bayesian model where each document is represented as a mixture of latent topics, and each topic is characterized by a distribution over words. The generative process assumes that documents exhibit a diverse set of topics, and each word in a document is generated by selecting a topic from the document's topic distribution and then selecting a specific word from the chosen topic's word distribution. The parameters governing these distributions are modeled using Dirichlet priors, providing a flexible and conjugate framework for probabilistic modeling. LDA has found extensive applications in information retrieval, document clustering, and topic discovery, offering insights into the thematic structure inherent in large corpora of textual data

## Supervised Latent Dirichlet Allocation

## Naive Bayes Classifier

## Latent Semantic Analysis

## Probabilistic Latent Semantic Analysis

# Motivation

## Data Definitions

* **Word-Label Matrix ($X$):** This matrix represents the observed data. Each entry $X_{i,j}$ indicates the number of times word $i$ appears in documents labeled as topic $j$. The value of $X_{i,j}$ is the count of word $i$ in documents labeled as topic $j$.

* **Notations:**
  * **m:** Number of topics
  * **n:** Number of words in the vocabulary
  * **d:** Number of documents

## Model

### Prior Distribution

The topic distribution for each word $i$ is modeled using a Dirichlet distribution, $\theta_i \sim Dirichlet(\textbf{a})$.  This is a natural choice for probability distributions over topics, ensuring that topic probabilities for each word sum to 1.The Dirichlet concentration parameter $\textbf{a}$ controls how sparse the topic distributions are. Lower values of $\textbf{a}$ encourage words to concentrate their probability mass on fewer topics, leading to more focused topic representations.

### Likelihood Function

The likelihood function models how likely it is to observe our word-label matrix $X$, given a particular set of topic distributions $\theta$. The core assumption is that words that appear frequently within a topic are more likely to be strongly associated with that topic. We define the likelihood function as follows:

$$\mathbb{P}(X | \theta) = \prod_{i=1}^n \left (\frac{\theta_{i,k} + \beta}{\sum_{k'=1}^{m} \theta_{i,k'} + \beta⋅m} \right)^{\alpha⋅\sum_{j=1}^{d} X_{i,j}}$$

The log-likelihood function is used for computational efficiency and numerical stability:

$$\log \mathbb{P}(X | \theta) = \sum_{i=1}^n \left (\alpha⋅\sum_{j=1}^{d} X_{i,j} \right) \log \left (\frac{\theta_{i,k} + \beta}{\sum_{k'=1}^{m} \theta_{i,k'} + \beta⋅m} \right)$$

* **Influence Parameter ($\alpha$):** This parameter scales the importance of word frequencies within topics. Higher values of $\alpha$ increase the association between topic $k$ and word $i$ if the word appears frequently in documents labeled with that topic.

* **Smoothing Parameter ($\beta$):** This parameter prevents zero probabilities and ensures robustness, particularly for words that might be rarely observed in the corpus.

### Inference

The model is implemented in Stan, a probabilistic programming language that allows us to express complex probabilistic models. Stan's Hamiltonian Monte Carlo (HMC) sampler is used to efficiently explore the posterior distribution of our model parameters, enabling robust inference of the latent topic structure.

### Hyperparameter Optimization

Hyperparameters ($\textbf{a}$, $\alpha$, $\beta$) are carefully tuned to improve the quality and coherence of the discovered topics. Techniques such as grid search or Bayesian optimization can be used to explore the hyperparameter space.

## Assumptions

# Case Study

The entire `Stan` code for this model and all the preprocessing code is provided in the appendix section. Below I will provide a pseudo-code for the model.

## Data Preprocessing

## Case 1: $\alpha = 0$, $\beta = 0$

## Case 2: $\alpha = 1$, $\beta = 0$

## Case 3: $\alpha = 1$, $\beta = 1$

## Case 3: $\alpha = 0.5$, $\beta = 0.5$

```{r, handle_data, echo=FALSE}
# Load the data
data <- read_csv("data/sentiment/sentiment.csv", show_col_types = FALSE) %>%
  select(text) %>%
  mutate(document_id = row_number()) %>%
  sample_n(10)

# Clean the data
data <- data %>%
  mutate(text = tolower(text)) %>%
  mutate(text = removeWords(text, stopwords("en"))) %>%
  mutate(text = removePunctuation(text,
    reserve_intra_word_contractions = TRUE,
    preserve_intra_word_dashes = TRUE
  )) %>%
  mutate(text = removeNumbers(text)) %>%
  mutate(text = str_replace_all(text, "�", "")) %>%
  mutate(text = str_replace_all(text, "\\s+", " ")) %>%
  mutate(text = str_trim(text))

# output the cleaned data to a text file
write_csv(data, "./cleaned_data.csv")

# Tokenization and stemming
data <- data %>%
  unnest_tokens(word, text) %>%
  mutate(word = wordStem(word))

# Create the word-document matrix with words as columns and documents as rows
X <- data %>%
  count(document_id, word) %>%
  cast_dtm(document_id, word, n) %>%
  as.matrix()
```

```{r, model, echo=FALSE}
model <- stan_model("model.stan")
```

```{r, inference, echo=FALSE, results='hide'}
stan_data <- list(
  n = ncol(X),
  d = nrow(X),
  X = X,
  m = 3,
  a = rep(0.5, 3),
  alpha = 1,
  beta = 0.5
)

fit <- sampling(model, data = stan_data, iter = 1000)
```

## Results

```{r, topic_word_dist, echo=FALSE}
theta <- extract(fit, "theta")$theta
theta_avg <- apply(theta, c(2, 3), mean)

topic_word_dist <- data.frame(
  word = colnames(X),
  topic = rep(seq_len(ncol(theta_avg)), each = nrow(theta_avg)),
  probability = c(theta_avg)
)

# order topic_word_dist by probability
# topic_word_dist %>%
#   arrange(desc(probability))

# plot a graph of the top words for each topic based on probability, with the words ordered by probability
top_words <- topic_word_dist %>%
  group_by(topic) %>%
  top_n(10, probability) %>%
  arrange(topic, desc(probability))

ggplot(
  top_words,
  aes(
    x = reorder(word, probability),
    y = probability,
    fill = factor(topic)
  )
) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_y_continuous(breaks = seq(0, 1, by = 0.05)) +
  labs(
    title = "Top 10 Words for Each Topic",
    x = "Word",
    y = "Probability",
    fill = "Topic"
  ) +
  theme_light() +
  facet_wrap(~topic, ncol = 1, scales = "free_y") +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_text(),
    strip.background = element_blank(),
    strip.text = element_blank(),
  ) +
  scale_fill_brewer(palette = "Set2")
```

# Discussion & Future Work

# Conclusion

# Appendix

## Stan Model

```{r, include=FALSE}
stan_code <- readLines("model.stan")
```

```{r, echo=FALSE}
cat(stan_code, sep = "\n")
```

## Case Study Data Preprocessing

# References