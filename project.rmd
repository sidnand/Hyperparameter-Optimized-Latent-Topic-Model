---
title: "A Hyperparameter-Optimized Latent Topic Model"
author: "Siddharth Nand"
bibliography: references.bib
output:
  pdf_document:
    latex_engine: xelatex
    toc: false
    number_sections: false
    fig_caption: true
    fig_height: 2
    fig_width: 5
    highlight: arrow
    df_print: kable
header-includes:
  - \usepackage{float}
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(stringr)
library(rstan)
library(tidytext)
library(textdata)
library(SnowballC)
library(tm)
library(caret)
library(topicmodels)
library(ggplot2)
library(magrittr)
library(mcmcse)
library(bayesplot)

setwd("/Users/siddharthnand/Library/CloudStorage/OneDrive-UBC/Undergrad Courses/W2023/STAT 477C Special Topics in Statistics - BAYESIAN STATS/Project")
```

# Introduction
Topic modeling uncovers hidden thematic structures within text corpora. It represents documents as mixtures of topics, where each topic is a probability distribution over words. This project introduces a novel topic modeling approach with a custom likelihood function and flexible parameterization. Users can adjust how word frequencies influence topics and the model's robustness to rare words.

# Literature Review

**Latent Dirichlet Allocation**

Latent Dirichlet Allocation (LDA) is a widely-used model for uncovering thematic structure in documents. It treats each document as a mix of topics, with each topic defined by a distribution over words. LDA's hierarchical Bayesian framework, using Dirichlet priors, offers flexibility in modeling. It has applications in information retrieval, document clustering, and topic discovery.

**Latent Semantic Analysis**

Latent Semantic Analysis (LSA) captures semantic relationships by representing documents and terms in a high-dimensional space and then reducing it. Utilizing singular value decomposition (SVD), LSA identifies similarities between documents and terms. While effective, it has limitations in handling polysemy and synonymy.

**Probabilistic Latent Semantic Analysis**

Probabilistic Latent Semantic Analysis (PLSA) extends LSA by adding a probabilistic framework. It models document generation as a process involving latent topics and observed words. PLSA learns topic structures iteratively, offering richer insights into document corpora despite limitations like requiring a predetermined number of topics.

# Problem Formulation

## Data Definitions

Our analysis begins with a word-label matrix $X$ which represents the observed word frequencies.  Each entry $X_{i,j}$  denotes the count of occurrences of word $i$ within documents labeled as belonging to topic $j$. For clarity, we'll use the following notations: $m$ represents the number of topics, $n$  the size of the vocabulary, and $d$ the total number of documents within the corpus.

## Model

Here's the probabilistic model used to infer the latent topics from the word-label matrix $X$.

### Prior Distribution

The topic distribution for each word $i$ is modeled using a Dirichlet distribution, $\theta_i \sim Dirichlet(\textbf{a})$. This distribution is ideally suited for topic modeling because it represents probability distributions over categories (like our topics). It ensures that topic probabilities for each word sum to 1 and allows words to have varying probabilities of belonging to multiple topics - reflecting the nuanced nature of language. The Dirichlet concentration parameter, $\textbf{a}$ (a vector with one value per topic), controls the sparsity of topic distributions. Higher values of $\textbf{a}$ lead to more even probability distributions across topics, resulting in broader topics. Conversely, lower values of $\textbf{a}$ encourage words to concentrate their probability mass on a smaller number of topics, yielding more distinct and focused thematic groups. This control over sparsity allows us to tailor the topic modeling process to different needs within a text analysis project. 

### Likelihood Function

The likelihood function models how likely it is to observe our word-label matrix $X$, given a particular set of topic distributions $\theta$. $\theta$ is a matrix where each row corresponds to a word and each column corresponds to a topic. The core assumption is that words that appear frequently within a topic are more likely to be strongly associated with that topic. We define the likelihood function as follows:

$$\mathbb{P}(X | \theta) = \prod_{i=1}^n \left (\frac{\theta_{i,k} + \beta}{\sum_{k'=1}^{m} \theta_{i,k'} + \beta⋅m} \right)^{\alpha⋅\sum_{j=1}^{d} X_{i,j}}$$

The log-likelihood function is used for computational efficiency and numerical stability:

$$\log \mathbb{P}(X | \theta) = \sum_{i=1}^n \left (\alpha⋅\sum_{j=1}^{d} X_{i,j} \right) \log \left (\frac{\theta_{i,k} + \beta}{\sum_{k'=1}^{m} \theta_{i,k'} + \beta⋅m} \right)$$

The model includes two key parameters that control how word frequencies and topic assignments are related. The influence parameter ($\alpha$)  scales how strongly a word's frequency within documents labeled with a topic affects its association with that topic.  The smoothing parameter ($\beta$) adds a small amount to word counts, preventing zero probabilities and making the model more robust, especially for less frequent words.

# Case Study

We'll apply our hyperparameter-optimized latent topic model to a real-world dataset, uncovering hidden thematic structures in a collection of financial news headlines. We'll compare our model to LDA and examine differences. The dataset comprises financial news headlines, and our goal is to organize them into coherent topics reflecting underlying themes. By fitting our model and plotting word-topic and document-topic distributions, we'll interpret the discovered themes based on top words in each topic. The `Stan` code and data preprocessing details are provided in the appendix.

## Data Preprocessing

```{r, handle_data, echo=FALSE}
set.seed(123)

# Load the data
data <- read_csv("data/sentiment/sentiment.csv", show_col_types = FALSE) %>%
  select(text)

# Convert data to utf-8
data$text <- iconv(data$text, to = "UTF-8")

# Clean the data
clean_data <- data %>%
  mutate(text = removeNumbers(text)) %>%
  mutate(text = removePunctuation(text,
    reserve_intra_word_contractions = TRUE,
    preserve_intra_word_dashes = TRUE,
    preserve_intra_word_underscore = TRUE
  )) %>%
  mutate(text = removeWords(text, stopwords("en"))) %>%
  mutate(text = str_replace_all(text, "�", "")) %>%
  mutate(text = str_replace_all(text, "\\s+", " ")) %>%
  mutate(text = str_replace_all(text, "’", "")) %>%
  mutate(text = str_replace_all(text, "\\b\\w{1,2}\\b", "")) %>%
  filter(text != "" | !is.na(text) | text != "NA") %>%
  mutate(text = tolower(text)) %>%
  mutate(text = str_trim(text)) %>%
  sample_n(25) %>%
  mutate(document_id = row_number())

# Tokenization and stemming
token_data <- clean_data %>%
  unnest_tokens(word, text) %>%
  mutate(word = wordStem(word))

# Create the word-document matrix with words as columns and documents as rows
X <- token_data %>%
  count(document_id, word) %>%
  cast_dtm(document_id, word, n) %>%
  as.matrix()
```

```{r, get_model, echo=FALSE, warning=FALSE, message=FALSE}
model <- stan_model("model.stan")
```

After preprocessing our data, we create a word-document matrix, with rows representing documents, columns representing unique words, and cells indicating word frequencies in each document. The preprocessing code is provided in the Appendix. Due to computational constraints, we randomly select a subset of 25 documents for analysis in our case study.

```{r, topic_word_dist, echo=FALSE}
plt_word_topic <- function(top_words) {
  ggplot(
  top_words,
  aes(
    x = term,
    y = probability,
    fill = factor(topic)
  )
) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    y = "Probability",
    fill = "Topic"
  ) +
  theme_light() +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.background = element_blank(),
    strip.text = element_blank(),
  ) +
  scale_fill_brewer(palette = "Set2")
}

```

```{r, inference, echo=FALSE, results='hide', warning=FALSE}
fit <- sampling(model,
  data = list(
    n = ncol(X),
    d = nrow(X),
    X = X,
    m = 3,
    a = rep(0.5, 3),
    alpha = 0.5,
    beta = 0.5
  ),
  iter = 1000,
  chains = 3
)

lda_model <- LDA(X, k = 3, method = "Gibbs", control = list(seed = 123))
```

## Comparison with LDA

Now we will compare our model to Latent Dirichlet Allocation (LDA) by plotting a word-topic distributions of the top 10 words which are the most probable for each topic. For our model, the hyperparameters $\alpha$ and $\beta$ were set to 0.5 and the Dirichlet concentration parameter $\textbf{a}$ was set to 0.5 for each topic. For LDA, we used the Gibbs sampling method with 3 topics. Figure 1 shows the word-topic distributions for our model, while Figure 2 shows the word-topic distributions for LDA. The y-axis is the probability of a word being in a specific topic. Also note that the words on the x-axis are after the stemming process (were we reduce words to their root form, e.g. "captures, capturing, captured" all become "captur"). The most noticeable difference is that the probability of a word belonging to a topic is more evenly distributed in our model compared to LDA and the value of those probabilities is higher. This is most likely due to the fact that our model only looks at the word frequencies within documents labeled with a topic, while LDA considers the entire corpus. Another difference between our models is the choice of words in each topic. It is expected that both models cluster words into topics in a different way, as they use different likelihood functions.

```{r, echo=FALSE, fig.cap="Hyperparameter-Optimized Word-Topic Distribution", fig.show='asis', warning=FALSE, fig.align='center', fig.pos='H', fig.width=6}
fit_summary <- summary(fit)
theta_means <- matrix(fit_summary$summary[, "mean"], nrow = ncol(X), ncol = 3, byrow = TRUE)

dist <- data.frame(
  term = colnames(X),
  topic = rep(1:3, each = ncol(X)),
  probability = as.vector(theta_means)
)

top_words <- dist %>%
  group_by(topic) %>%
  arrange(desc(probability)) %>%
  slice(1:10)

plt_word_topic(top_words)
```

```{r, echo=FALSE, fig.cap="LDA Word-Topic Distribution", fig.show='asis', warning=FALSE, fig.align='center', fig.pos='H', fig.width=6}
lda_topics <- tidy(lda_model, matrix = "beta")
top_words <- lda_topics %>%
  group_by(topic) %>%
  arrange(desc(beta)) %>%
  slice(1:10)

top_words <- top_words %>%
  rename(probability = beta)

plt_word_topic(top_words)
```

## Effective Sample Size

For our model, the parameter of interest, $\theta$ is a matrix. So for each chain, we will have an effective sample size (ESS) for each parameter in the matrix. We can make a boxplot of the ESS for each parameter in $\theta$ per chain. In Figure 3, we can see that the ESS values are quite high, indicating that the chains have mixed well and that the samples are independent. This is important for the validity of our inference.
```{r, echo=FALSE, fig.cap="Effective Sample Size per Chain", fig.show='asis', fig.width=3, fig.height=2.5, warning=FALSE, fig.align='center', fig.pos='H'}
# Extract the effective sample size (ESS) values
ess_values <- fit_summary$summary[, "n_eff"]

n_words <- ncol(X)
n_chains <- 3
n_topics <- 3

# Calculate the total number of parameters and chains
num_parameters <- n_words * n_topics  # Number of words * Number of topics
num_chains <- n_chains       # Number of chains

# Calculate the total number of iterations
num_iterations <- nrow(fit_summary$summary)

# Calculate the expected length of ess_values
expected_length <- num_parameters * num_chains

# Adjust expected_length to match the actual length of ess_values
if (expected_length != length(ess_values)) {
  ess_values <- ess_values[1:expected_length]
}

ess_per_chain <- matrix(ess_values, nrow = num_parameters, ncol = num_chains, byrow = TRUE)

# Assuming ess_per_chain is your matrix of ESS values
# Convert ess_per_chain into a data frame
ess_df <- data.frame(chain = rep(1:num_chains, each = num_parameters),
                      parameter = rep(1:num_parameters, times = num_chains),
                      ess = as.vector(ess_per_chain))

ggplot(ess_df, aes(x = factor(chain), y = ess)) +
  geom_boxplot() +
  xlab("Chain") +
  ylab("ESS") +
  theme_minimal()

```

## Trace Plots

We can also plot the trace plots for each parameter in $\theta$ per chain. However, due to the large number of parameters, we will only plot the trace plots for the first 3 parameters in $\theta$ for each topic per chain. In Figure 4, we can see that as the number of iterations increases, the parameter values do not show any clear trend, indicating that the chains have mixed well and that the samples are independent. This is important for the validity of our inference.

```{r, echo=FALSE, fig.cap="Trace Plots per Chain", fig.show='asis', fig.width=7, fig.height=4, warning=FALSE, fig.align='center', fig.pos='H'}
pars <- c()
for (i in 1:3) {
  for (j in 1:3) {
    pars <- c(pars, paste("theta[", i, ",", j, "]", sep = ""))
  }
}

mcmc_trace(fit, pars = pars) +
  theme_minimal() +
  theme(plot.margin = margin(0.25, 0.25, 0.25, 0.25, "cm"))
```

# Discussion \& Conclusion

From our case study, we can see that the inference behavior of our model is good, with Figure 3 and 4 showing high effective sample sizes and well-mixed chains. The word-topic distributions in Figure 1 show that our model clusters words into topics differently than LDA, with more evenly distributed probabilities. Due to the unsupervised nature of topic modeling, it is difficult to evaluate the quality of the topics discovered.

Our model is not without limitations. The choice of hyperparameters $\alpha$ and $\beta$ can significantly impact the model's performance. The Dirichlet concentration parameter $\textbf{a}$ can also affect the sparsity of the topic distributions because it controls how words are distributed across topics. Because our parameter of interest, $\theta$, is a matrix, the number of parameters can be quite large, which can impact the computational efficiency of the model. Fitting our data matrix of 25 documents took about 5mins as-opposed to LDA which took about a few seconds. Thus our model will be difficult to scale to larger datasets. Another large limitation to our model is our liklihood function. It is quite simple and does not take into account the order of words in a document or the context in which they appear. This can lead to topics that are not coherent or meaningful.

\newpage
# Appendix

## Data Preprocessing

```{r, handle_data, echo=TRUE}
```

\newpage
## Model Code

```{stan, output.var="model", echo=TRUE}
data {
  int<lower=1> n;  // Number of words
  int<lower=1> d;  // Number of documents
  int<lower=0> X[d, n]; // Word-document matrix

  int<lower=1> m;  // Number of topics

  vector<lower=0>[m] a; // Dirichlet concentration parameter
  real<lower=0> alpha;  // Word frequency influence
  real<lower=0> beta;   // Smoothing parameter
}

parameters {
  simplex[m] theta[n];
}

model {
  for (i in 1:n) {
    theta[i] ~ dirichlet(a);
  }

  for (i in 1:n) {
    for (j in 1:d) {
      for (k in 1:m) {

        real w = (alpha * X[j, i]);
        real u = log((theta[i, k] + beta) / (sum(theta[i]) + beta * m));
        target += w * u;

      }
    }
  }
}

```

\newpage
# References