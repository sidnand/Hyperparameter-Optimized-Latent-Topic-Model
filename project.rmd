---
title: "Learning from Labels: A Supervised Approach to Topic Modeling"
author: "Siddharth Nand"
bibliography: references.bib
output: 
    pdf_document:
        latex_engine: xelatex
        toc: false
        number_sections: false
        fig_caption: true
        fig_height: 5
        fig_width: 7
        highlight: tango
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(stringr)

setwd("/Users/siddharthnand/Library/CloudStorage/OneDrive-UBC/Undergrad Courses/W2023/STAT 477C Special Topics in Statistics - BAYESIAN STATS/Project")
```

# Introduction
Latent topic modelling is an unsupervised tool for uncovering the underlying themes in a collection of documents. In this project, I propose a method of Latent Topic Modeling for supervised learning. The goal is to develop a model that given an article, can identify the probability of the article belonging to each of the predefined themes.

# Literature Review

## Latent Dirichlet Allocation
Latent Dirichlet Allocation (LDA) is a widely used probabilistic generative model in natural language processing for uncovering the latent thematic structure within a collection of documents. First proposed in the context of population genetics [@Pritchard2000; @Falush2003], LDA posits a hierarchical Bayesian model where each document is represented as a mixture of latent topics, and each topic is characterized by a distribution over words. The generative process assumes that documents exhibit a diverse set of topics, and each word in a document is generated by selecting a topic from the document's topic distribution and then selecting a specific word from the chosen topic's word distribution. The parameters governing these distributions are modeled using Dirichlet priors, providing a flexible and conjugate framework for probabilistic modeling. LDA has found extensive applications in information retrieval, document clustering, and topic discovery, offering insights into the thematic structure inherent in large corpora of textual data

# Motivation

# Methodology

## Data Definitions

Let $m$ be the number of topics (labels) and let $n$ be the number of words in the vocabulary. Let $X$ be the word-label matrix of size $n \times d$ where $x_{i, j}$ is the number of times word $i$ appears in documents with label $j$. This is our observed data matrix. Finally, we will let $y_k$ be the $k^{th}$ topic. These are the latent variables we want to infer. Also note that $k = m$ because we have $m$ topics that we want to infer.

## Model

**Prior:** Let $y_i \sim Uniform[0, k]$ be a label. So initally, we assume that each word is equally likely to belong to any of the $m$ topics.

Your explanation is quite clear, but let's refine it further for better clarity:

**Likelihood:** The likelihood function $\mathbb{P}(X | y_k)$ models the probability that a word $i$ belongs to label $j$ given the observed data matrix $X$. For us to make a clear relation between labels and words, we need a way to say that "the more frequently we observe more word $i$ with label $j$, then there is a higher probability that word $i$ should be labeled $j$." To acknowledge this relationship, we will assume that as we see more words with label $j$, the probability that word $i$ belongs to label $j$ increases linearly by some tuning parameter $\alpha$, which controls the rate of increase in label probabilities based on word frequencies. We will also let $\theta_{i, k}$ be the count of word $i$ in the $k^{th}$ topic, this is the parameter we want to infer.

The likelihood function can be expressed as:

$$\mathbb{P}(X | y_k) = \prod_{i=1}^{n} \left(1 + \alpha \cdot \sum_{j=1}^{d} X_{i,j}\right)^{\theta_{i,k}}$$

Step-by-step explanation of the likelihood function:

- For each word $i$, we calculate a factor that represents the likelihood of that word belonging to label $j$.
- This factor is proportional to the count of times word $i$ appears in documents with label $j$, stored in $X_{i,j}$. This is adjusted by the tuning parameter $\alpha$.
- By multiplying these factors for all words in the vocabulary, we obtain the likelihood of observing the entire word-label matrix $X$ given the topic $y_k$.

This likelihood function allows us to model the relationship between words and labels, considering both the observed word-label counts and the influence of word frequencies on label probabilities. By maximizing this likelihood function and solving for $\theta_{i,k}$ over all topics $y_k$, we can infer the most likely topics for the observed word-label matrix $X$. Additionally, the model accounts for unseen words by ensuring that the probability remains non-zero when $X_{i, j} = 0$, providing robustness to unseen data.

## Assumptions

In the likelihood function described, several assumptions are made to facilitate the modeling process. Let's outline these assumptions:

1. **Word Independence within Documents**: Although the likelihood function considers the counts of individual words within documents, it assumes that the occurrence of each word in the document is independent of the occurrence of other words. This assumption simplifies the modeling process and allows for efficient computation, but it might not hold true in all cases, especially for documents with strong contextual dependencies between words.

2. **Linear Relationship between Word Counts and Label Probabilities**: The likelihood function assumes a linear relationship between the counts of words in documents with specific labels and the probabilities of those labels. This implies that as the count of a word increases in documents with a certain label, the probability of that label being assigned to a new document containing that word also increases linearly. While this assumption provides a straightforward mechanism for updating label probabilities based on observed word frequencies, it might oversimplify the true relationship between word occurrences and label assignments.

3. **Uniform Prior Distribution of Labels**: The model assumes a uniform prior distribution of labels, meaning that each label has an equal initial probability of being assigned to a document in the absence of any observed evidence. While this simplifies the model initialization, it might not accurately reflect the true distribution of labels in the dataset. In practice, label priors could be estimated from the data or incorporated based on domain knowledge.

4. **Fixed Tuning Parameter $\alpha$**: The likelihood function includes a tuning parameter $\alpha$ that controls the rate of increase in label probabilities based on word frequencies. This parameter is fixed and assumed to be constant across all words and labels. However, in reality, the optimal value of $\alpha$ might vary depending on the characteristics of the dataset and the specific task at hand. 

# Case Study

# Discussion & Future Work

# Conclusion

# References  